Regression : 회귀 (값을 예측)
Classification : 분류 (분류)

===================================== 학습이란 =====================================
컴퓨터에 있어서 학습이란?
(분류기를 학습시키는 것에 집중한 개념임 / regression과 classification 결국 다 분류기임)
1. 임의로 hypothesis를 세운다.
2. cost function을 통해 실제 데이터와의 차이(에러)를 구한다.
3. Optimizer를 통해 에러를 줄여가는 w, b 값을 구해 업데이트한다.
4. 아주 최적의 hypothesis를 만든다.
5. 실제 데이터에 hypothesis가 분류기로 사용된다.
===================================================================================





=================================== Regression ====================================
Linear 즉, 입력된 데이터를 가지고 한 선을 통해 결과를 예측하는 것임.
입력값의 변수가 많느냐 적느냐의 차이임. 둘은

1. Linear Regression

Hypothesis : y=wx+b
Cost function : Sum Squared error
Optimizer : Gradient Descent Algorithm


2. Multi-variable linear Regression

Hypothesis : y=w1x1+w2x2+ ... +b
Cost function : Sum Squared error
Optimizer : Gradient Descent Algorithm
===================================================================================





================================= Classification ==================================
0 or 1의 분류의 문제이므로 Linear한 선으로 분류가 어려움.(하지만 linear한 선이 뿌리가 됨 이 linear한 선을 구부리는 것이기 때문)
그래서 값을 활성화 함수에 넣어 0 ~ 1 사이의 값으로 뽑아내어 0.5를 기준으로 분류.

1. Logitsic Classification

Hypothesis : sigmoid(H(X))
Cost function : -ylog(H(x))-(1-y)log(1-H(x))
Optimizer : Gradient Descent Algorithm

2. Multinomial Classification(주로 Softmax Classification)

Hypothesis : softmax(H(X))
Cost function : Cross-entropy(-ylog(H(x))-(1-y)log(1-H(x)) 과 거의 동일)
Optimizer : Gradient Descent Algorithm
===================================================================================





============================= Softmax Classification ==============================
■ Softmax Classification
Hypothesis : softmax(H(X))
Cost function : Cross-entropy(-ylog(H(x))-(1-y)log(1-H(x)) 과 거의 동일)
Optimizer : Gradient Descent Algorithm

-이론-
softmax classfication은 여러개의 클래스가(분류 공간이 여러개) 있을 때 분류기로 사용하기 좋다.
softmax function이 하는 역할은 단지 여러개의 각 클래스를 위한 linear한 분류기로 측정한 값을
softmax function에 집어 넣고 여기서 집어넣어진 측정 값들 각각을 0~1 사이의 값으로 총 합이 1이 되도록 만들어 주는 함수이다.
이렇게 나온 값을 One-hot Encoding 하면 분류기의 분류결과가 나오게 되는 것이다.

-물리-
웨이트가 클래스가 늘어나는만큼 배로 늘어난다. (각 클래스를 위한 분류기가 생성되어야 하므로)

*softmax 함수는 왜 제일 큰 값을 선택하게 되는가?
당연한 원리임 w값과 입력값이 곱해져 결과가 나오고
이게 적절할수록 w에 빼지는 값이 작아지므로 결과적으로 값이 높은게 가능성이 당연히 더 큰 것임
===================================================================================